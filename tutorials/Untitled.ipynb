{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Plot of Noam Annealing\n",
    "###\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "d_model = 256\n",
    "warmup = 4000\n",
    "max_steps = 20000\n",
    "lr = 1\n",
    "min_lr = 1e-5\n",
    "\n",
    "def _noam_annealing(initial_lr, step):\n",
    "    normalize = d_model ** (-0.5)\n",
    "    warmup_steps = warmup\n",
    "    mult = normalize * min(step ** (-0.5), step * (warmup_steps ** (-1.5)))\n",
    "    out_lr = initial_lr * mult\n",
    "    if step > warmup_steps:\n",
    "        out_lr = max(out_lr, min_lr)\n",
    "    return out_lr\n",
    "\n",
    "x = range(1, max_steps)\n",
    "y = []\n",
    "for i in x:\n",
    "    y.append(_noam_annealing(lr, i))\n",
    "    if i % 1000 == 0:\n",
    "        print(_noam_annealing(lr, i))\n",
    "plt.plot(x, y)\n",
    "plt.show()\n",
    "\n",
    "class NoamAnnealing(object):\n",
    "    def __init__(\n",
    "        self, d_model, max_steps, warmup_steps=None, min_lr=0.0, last_epoch=-1\n",
    "    ):\n",
    "        self._normalize = d_model ** (-0.5)\n",
    "\n",
    "        self.max_steps = max_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.min_lr = min_lr\n",
    "\n",
    "    def get_lr(self, step, lr):\n",
    "        if step > self.max_steps:\n",
    "            return [self.min_lr for _ in self.base_lrs]\n",
    "\n",
    "        new_lrs = self._noam_annealing(initial_lr=lr, step=step)\n",
    "        return new_lrs\n",
    "\n",
    "    def _noam_annealing(self, initial_lr, step):\n",
    "        mult = self._normalize * min(step ** (-0.5), step * (self.warmup_steps ** (-1.5)))\n",
    "        out_lr = initial_lr * mult\n",
    "        if step > self.warmup_steps:\n",
    "            out_lr = max(out_lr, self.min_lr)\n",
    "        return out_lr\n",
    "\n",
    "d_model=256\n",
    "max_steps=20000\n",
    "warmup_steps=4000\n",
    "min_lr=1e-5\n",
    "lr = 1\n",
    "\n",
    "myanneal = NoamAnnealing(d_model=d_model, max_steps=max_steps, warmup_steps=warmup_steps, min_lr=min_lr)\n",
    "x = range(1, max_steps)\n",
    "y = []\n",
    "for i in x:\n",
    "    y.append(myanneal.get_lr(i, lr))\n",
    "    if i % 1000 == 0:\n",
    "        print(myanneal.get_lr(i, lr))\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###\n",
    "# F0 Checks\n",
    "###\n",
    "\n",
    "from pathlib import Path\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import json\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "audio = []\n",
    "all_data = []\n",
    "total_duration = 0\n",
    "with open(\"/data/speech/NvidiaCustomTTSFinetuning/trimmed_sirisha35/manifest35.json\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    np.random.shuffle(lines)\n",
    "    for lin in lines:\n",
    "        data = json.loads(lin)\n",
    "        total_duration += data['duration']\n",
    "        if total_duration > 45*60:\n",
    "            break\n",
    "        audio.append(data['audio_filepath'])\n",
    "        all_data.append(data)\n",
    "print(f\"Total duration: {total_duration}\")\n",
    "\n",
    "with open(\"/data/speech/NvidiaCustomTTSFinetuning/trimmed_sirisha35/manifest35_45mins.json\", \"w\") as f:\n",
    "    for i, line in enumerate(all_data):\n",
    "        f.write(json.dumps(line)+'\\n')\n",
    "\n",
    "# audio = []\n",
    "# total_duration = 0\n",
    "# with open(\"/data/speech/NvidiaCustomTTSFinetuning/trimmed_sirisha35/manifest35_15mins.json\", \"r\") as f:\n",
    "#     lines = f.readlines()\n",
    "#     for lin in lines:\n",
    "#         data = json.loads(lin)\n",
    "#         audio.append(data['audio_filepath'])\n",
    "    \n",
    "\n",
    "# pitch_fmin = 64 # REALLY Should change this\n",
    "# pitch_fmax = 512 # Should change this\n",
    "# n_window_size=2048  # Don't have to change this\n",
    "# # all_f0 = np.array([])\n",
    "# for i, f in enumerate(audio):\n",
    "#     y, _ = librosa.core.load(f, sr=44100)\n",
    "#     stft_matrix = librosa.amplitude_to_db(np.abs(librosa.stft(y, n_fft=2048, window=\"hann\")))\n",
    "#     f0, voiced_flag, voiced_probs = librosa.pyin(y, fmin=pitch_fmin, fmax=pitch_fmax, frame_length=n_window_size, sr=44100, fill_na=0.)\n",
    "    \n",
    "#     ## UNCOMMENT THIS SECTION TO EXPERIMENT WITH FMIN, FMAX\n",
    "# #     fig, ax = plt.subplots()\n",
    "# #     librosa.display.specshow(stft_matrix, sr=44100, hop_length=512, y_axis=\"log\", ax=ax)\n",
    "# #     ax.plot(f0, label='f0', color='cyan', linewidth=3)\n",
    "\n",
    "#     # Hack to remove off_pitches\n",
    "#     found_flat_area = False\n",
    "#     start = 0\n",
    "#     end = 0\n",
    "#     for i, pitch in enumerate(f0):\n",
    "#         if pitch==0.:\n",
    "#             if found_flat_area:\n",
    "#                 end = i\n",
    "#                 # Decide whether to trim this\n",
    "#                 voice_prob_avg = np.mean(voiced_probs[start:end])\n",
    "#                 if voice_prob_avg < 0.03:\n",
    "#                     # Delete pitch\n",
    "#                     f0[start:end] = 0.\n",
    "#                 else:\n",
    "#                     # Probably started real audio\n",
    "#                     break\n",
    "#                 found_flat_area = False\n",
    "#                 start = 0\n",
    "#             continue\n",
    "#         elif not found_flat_area:\n",
    "#             found_flat_area = True\n",
    "#             start = i\n",
    "#             end = 0\n",
    "#     found_flat_area = False\n",
    "#     start = 0\n",
    "#     end = 0\n",
    "#     for i, pitch in enumerate(reversed(f0)):\n",
    "#         if pitch==0.:\n",
    "#             if found_flat_area:\n",
    "#                 end = i\n",
    "#                 # Decide whether to trim this\n",
    "#                 voice_prob_avg = np.mean(voiced_probs[(len(f0)-end):(len(f0)-start)])\n",
    "#                 if voice_prob_avg < 0.03:\n",
    "#                     # Delete pitch\n",
    "#                     f0[(len(f0)-end):(len(f0)-start)] = 0.\n",
    "#                 else:\n",
    "#                     # Probably started real audio\n",
    "#                     break\n",
    "#                 found_flat_area = False\n",
    "#                 start = 0\n",
    "#             continue\n",
    "#         elif not found_flat_area:\n",
    "#             found_flat_area = True\n",
    "#             start = i\n",
    "#             end = 0\n",
    "\n",
    "# ## UNCOMMENT THIS SECTION TO EXPERIMENT WITH FMIN, FMAX\n",
    "#     all_f0 = np.concatenate((all_f0,f0[f0!=0].flatten()))\n",
    "#     ax.plot(f0, label='f0', color='cyan', linewidth=2)\n",
    "#     ax.plot(voiced_probs*500, label='f0', linewidth=3)\n",
    "# #     plt.show()\n",
    "\n",
    "# # ### UNCOMMENT THIS SECTION WHEN YOU FINALIZE FMIN, FMAX\n",
    "# #     filename = f\"/data/speech/NvidiaCustomTTSFinetuning/trimmed_sirisha35/pitch_prior/{Path(f).stem}_pitch_pyin_fmin{pitch_fmin}_fmax{pitch_fmax}_fl{n_window_size}.npy\"\n",
    "# #     np.save(filename, f0)\n",
    "\n",
    "# ## WE STILL NEED TO FIND MEAN AND STD FOR EACH 15min, 30min, etc. manifest\n",
    "# print(f\"pitch mean: {np.mean(all_f0)}\")\n",
    "# print(f\"pitch std: {np.std(all_f0)}\")\n",
    "    \n",
    "# # import torch\n",
    "\n",
    "# # y, _ = librosa.core.load(\"/data/speech/LJSpeech/wavs/LJ030-0200.wav\", sr=22050)\n",
    "# # stft_matrix = librosa.amplitude_to_db(np.abs(librosa.stft(y, n_fft=1024, window=\"hann\", hop_length=256)))\n",
    "# # %matplotlib inline\n",
    "# # fig, ax = plt.subplots()\n",
    "# # librosa.display.specshow(stft_matrix, sr=22050, hop_length=256, y_axis=\"log\", ax=ax)\n",
    "# # f0 = torch.load(\"/data/speech/LJSpeech/supplementary/LJ030-0200_melodia_f0min80_f0max800_harm1.0_mps0.0.pt\")[\"f0\"]\n",
    "# # f0 = f0.numpy().squeeze()\n",
    "# # f0[f0==0] = np.nan\n",
    "# # ax.plot(f0, label='f0', color='cyan', linewidth=3)\n",
    "\n",
    "# # import pysptk\n",
    "\n",
    "# # y, _ = librosa.core.load(\"/data/speech/LJSpeech/wavs/LJ030-0200.wav\", sr=22050)\n",
    "# # stft_matrix = librosa.amplitude_to_db(np.abs(librosa.stft(y, n_fft=1024, window=\"hann\", hop_length=256)))\n",
    "# # %matplotlib inline\n",
    "# # fig, ax = plt.subplots()\n",
    "# # librosa.display.specshow(stft_matrix, sr=22050, hop_length=256, y_axis=\"log\", ax=ax)\n",
    "# # f0 = pysptk.rapt(y.astype(np.float32) * 32768, fs=22050, hopsize=256, otype=\"f0\")\n",
    "# # f0[f0==0] = np.nan\n",
    "# # ax.plot(f0, label='f0', color='cyan', linewidth=3)\n",
    "\n",
    "# # librosa.display.specshow(stft_matrix, sr=22050, hop_length=256, y_axis=\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# GL\n",
    "###\n",
    "\n",
    "import librosa\n",
    "def griffin_lim(magnitudes, n_iters=50, n_fft=1024):\n",
    "    \"\"\"\n",
    "    Griffin-Lim algorithm to convert magnitude spectrograms to audio signals\n",
    "    \"\"\"\n",
    "    phase = np.exp(2j * np.pi * np.random.rand(*magnitudes.shape))\n",
    "    complex_spec = magnitudes * phase\n",
    "    signal = librosa.istft(complex_spec)\n",
    "    if not np.isfinite(signal).all():\n",
    "        logging.warning(\"audio was not finite, skipping audio saving\")\n",
    "        return np.array([0])\n",
    "\n",
    "    for _ in range(n_iters):\n",
    "        _, phase = librosa.magphase(librosa.stft(signal, n_fft=n_fft))\n",
    "        complex_spec = magnitudes * phase\n",
    "        signal = librosa.istft(complex_spec)\n",
    "    return signal\n",
    "\n",
    "linear = spec.cpu().numpy().squeeze(0)\n",
    "linear = np.dot(librosa.filters.mel(44100, 2048, n_mels=80, fmin=0.0, fmax=None).T, linear)\n",
    "linear = np.clip(linear, a_min=0, a_max=255)\n",
    "audio = griffin_lim(linear**1.5, n_fft=2048)\n",
    "\n",
    "ipd.Audio(audio, rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###\n",
    "# Inference (T2+HiFiGAN)\n",
    "###\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "\n",
    "from nemo.collections.asr.data.audio_to_text import _AudioTextDataset\n",
    "from nemo.collections.tts.models import Tacotron2Model, FastPitchModel\n",
    "from nemo.collections.tts.modules.hifigan_modules import Generator\n",
    "\n",
    "\n",
    "# t2 = Tacotron2Model.restore_from(\"/home/jasoli/nemo/NeMo/examples/tts/Tacotron2.nemo\")\n",
    "# t2 = Tacotron2Model.restore_from(\"/home/jasoli/nemo/NeMo/Tacotron2-Helen-char.nemo\")\n",
    "t2 = Tacotron2Model.restore_from(\"/home/jasoli/nemo/NeMo/Tacotron2-large-9017.nemo\")\n",
    "t2 = t2.cuda().eval()\n",
    "t2.decoder.gate_threshold = 0.4\n",
    "labels = t2.cfg.labels\n",
    "\n",
    "vocoder = Generator(\n",
    "    resblock=1,\n",
    "    upsample_rates=[8, 8, 4, 2],\n",
    "    upsample_kernel_sizes=[16, 16, 4, 4],\n",
    "    upsample_initial_channel=512,\n",
    "    resblock_kernel_sizes=[3, 7, 11],\n",
    "    resblock_dilation_sizes=[[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n",
    ")\n",
    "nemo_gen_keys = [k for k in vocoder.state_dict().keys()]\n",
    "# adlr_gen_ckpt = torch.load(\"/home/jasoli/nemo/NeMo/Hifigan_ADLR_male_44k_00632500\")\n",
    "adlr_gen_ckpt = torch.load(\"/home/jasoli/nemo/NeMo/Hifigan_ADLR_male_44k_00632500.pt\")[\"generator\"]\n",
    "# adlr_gen_ckpt = torch.load(\"/home/jasoli/nemo/NeMo/Hifigan_ADLR_Helen_44k_00570000.pt\")[\"generator\"]\n",
    "adlr_gen_keys = adlr_gen_ckpt.keys()\n",
    "\n",
    "new_nemo_ckpt = {nemo_key: adlr_gen_ckpt[adlr_key] for adlr_key, nemo_key in zip(adlr_gen_keys, nemo_gen_keys)}\n",
    "vocoder.load_state_dict(new_nemo_ckpt)\n",
    "vocoder = vocoder.cuda().eval().half()\n",
    "\n",
    "dataset = _AudioTextDataset(\n",
    "     \"/data/speech/HiFiTTS/9017_manifest_clean_dev.json\",\n",
    "#     \"/data/speech/Helen/helen_val.json\",\n",
    "#     \"/mnt/ssd1/data/LJSpeech-1.1/nvidia_ljspeech_test.json\",\n",
    "    parser=t2.parser,\n",
    "    sample_rate=44100,\n",
    "    bos_id=len(labels),\n",
    "    eos_id=len(labels) + 1,\n",
    "    pad_id=len(labels) + 2,\n",
    "    max_utts=5,\n",
    "    min_duration=1.,\n",
    "    max_duration=16.,\n",
    ")\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset, batch_size=1, collate_fn=dataset.collate_fn, num_workers=0, shuffle=False\n",
    ")\n",
    "device = torch.device(\"cuda\")\n",
    "all_utterances = 0\n",
    "all_samples = 0\n",
    "\n",
    "batches = []\n",
    "for b in dataloader:\n",
    "    batches.append(b)\n",
    "\n",
    "def infer_vocoder(model, spec: torch.Tensor):\n",
    "    audio = model(x=spec).squeeze(1)\n",
    "    return audio\n",
    "\n",
    "audios = []\n",
    "gt_audios = []\n",
    "for batch in batches:\n",
    "    gt_audio, _, text, text_length = batch\n",
    "    gt_audios.append(gt_audio)\n",
    "    text = text.to(device)\n",
    "    text_length = text_length.to(device)\n",
    "    with torch.no_grad():\n",
    "        spectrogram = t2.generate_spectrogram(tokens=text)\n",
    "        audios.append(infer_vocoder(model=vocoder, spec=spectrogram.half()))\n",
    "\n",
    "import IPython.display as ipd\n",
    "for i, (aud, gt_aud) in enumerate(zip(audios, gt_audios)):\n",
    "    print(f\"Sample_{i}\")\n",
    "    ipd.display(ipd.Audio(aud.float().cpu().numpy(), rate=44100))\n",
    "    ipd.display(ipd.Audio(gt_aud.float().cpu().numpy(), rate=44100))\n",
    "    write(f\"sample_{i}.wav\", 44100, aud.float().cpu().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Splicing rap mp3  - did not work\n",
    "###\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "\n",
    "\n",
    "y, sr = librosa.core.load(\"./3899_Eminem - The Real Slim Shady (Acapella).mp3\", sr=None)\n",
    "\n",
    "start = int((60+21.7)*sr)\n",
    "# end = int((60+30.58)*sr)  # end of please stand up x3\n",
    "end = int((60+28.2)*sr)  # end of first please stand up \n",
    "spliced = y[start:end]\n",
    "ipd.display(ipd.Audio(spliced, rate=sr))\n",
    "\n",
    "stft_matrix = librosa.amplitude_to_db(np.abs(librosa.stft(spliced, n_fft=2048, window=\"hann\")))\n",
    "f0, voiced_flag, voiced_probs = librosa.pyin(spliced, fmin=120, fmax=512, frame_length=2048, sr=sr)\n",
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "librosa.display.specshow(stft_matrix, sr=sr, hop_length=512, y_axis=\"log\", ax=ax)\n",
    "ax.plot(f0, label='f0', color='cyan', linewidth=3)\n",
    "\n",
    "sf.write('chorus.wav', spliced, sr)\n",
    "\n",
    "f0_mean = np.mean(f0[~np.isnan(f0)])\n",
    "f0_std = np.std(f0[~np.isnan(f0)])\n",
    "\n",
    "f0_stand = (f0 - f0_mean)/f0_std\n",
    "f0_stand[np.isnan(f0_stand)] = 0.\n",
    "\n",
    "# Load fastpitch model\n",
    "\n",
    "from nemo.collections.tts.models import FastPitchModel\n",
    "import torch\n",
    "from nemo.collections.tts.torch.helpers import beta_binomial_prior_distribution\n",
    "from nemo.utils import logging\n",
    "logging.setLevel(logging.ERROR)\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "\n",
    "fastpitch = FastPitchModel.restore_from(\"/home/jasoli/nemo/NeMo/FastPitch-Align-8051.nemo\").eval()\n",
    "\n",
    "# inp = (\"I'm Slim Shady, yes, I'm the real Shady. \"\n",
    "#        \"All you other Slim Shadys are just imitating. \"\n",
    "#        \"So won't the real Slim Shady please stand up?\")\n",
    "# tokens = fastpitch.parse(inp)\n",
    "\n",
    "# mels, spec_len = fastpitch.preprocessor(input_signal=torch.tensor(spliced).unsqueeze(0).cuda(), length=torch.tensor([len(spliced)]).cuda())\n",
    "\n",
    "# mels_pred, _, durs_predicted, log_durs_pred, pitch_pred, attn_soft, attn_logprob, attn_hard, attn_hard_dur, pitch = fastpitch(\n",
    "#     text=tokens,\n",
    "#     durs=None,\n",
    "#     pitch=None,\n",
    "#     speaker=None,\n",
    "#     pace=1.0,\n",
    "#     spec=mels,\n",
    "#     attn_prior=torch.tensor(beta_binomial_prior_distribution(tokens.shape[1],mels.shape[2])).unsqueeze(0).cuda(),\n",
    "#     mel_lens=spec_len,\n",
    "#     input_lens=torch.tensor(tokens.shape[1]).unsqueeze(0),\n",
    "# )\n",
    "\n",
    "# spec, *_ = fastpitch(\n",
    "#     text=tokens,\n",
    "#     durs=new_dur.cuda(),\n",
    "#     pitch=torch.tensor(f0_stand).cuda().unsqueeze(0),\n",
    "#     speaker=None,\n",
    "#     pace=1.0,\n",
    "#     spec=None,\n",
    "#     attn_prior=None,\n",
    "#     mel_lens=None,\n",
    "#     input_lens=torch.tensor(tokens.shape[1]).unsqueeze(0),\n",
    "# )\n",
    "\n",
    "# spec_2, *_ = fastpitch(\n",
    "#     text=tokens,\n",
    "#     durs=attn_hard_dur,\n",
    "#     pitch=None,\n",
    "#     speaker=None,\n",
    "#     pace=1.0,\n",
    "#     spec=None,\n",
    "#     attn_prior=None,\n",
    "#     mel_lens=None,\n",
    "#     input_lens=torch.tensor(tokens.shape[1]).unsqueeze(0),\n",
    "# )\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[15,8])\n",
    "ax.imshow(mels.cpu().detach().numpy()[0], origin=\"lower\")\n",
    "plt.show()\n",
    "\n",
    "from nemo.collections.tts.modules.hifigan_modules import Generator\n",
    "\n",
    "vocoder = Generator(\n",
    "    resblock=1,\n",
    "    upsample_rates=[8, 8, 4, 2],\n",
    "    upsample_kernel_sizes=[16, 16, 4, 4],\n",
    "    upsample_initial_channel=512,\n",
    "    resblock_kernel_sizes=[3, 7, 11],\n",
    "    resblock_dilation_sizes=[[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n",
    ")\n",
    "nemo_gen_keys = [k for k in vocoder.state_dict().keys()]\n",
    "adlr_gen_ckpt = torch.load(\"/home/jasoli/nemo/NeMo/hifigan_gen_nemo_8051_00524000.pt\")\n",
    "adlr_gen_keys = adlr_gen_ckpt.keys()\n",
    "\n",
    "new_nemo_ckpt = {nemo_key: adlr_gen_ckpt[adlr_key] for adlr_key, nemo_key in zip(adlr_gen_keys, nemo_gen_keys)}\n",
    "vocoder.load_state_dict(new_nemo_ckpt)\n",
    "vocoder = vocoder.cuda().eval()\n",
    "\n",
    "\n",
    "audio = vocoder(x=spec)\n",
    "audio2 = vocoder(x=mels_pred)\n",
    "\n",
    "ipd.display(ipd.Audio(audio.cpu().detach().numpy()[0], rate=44100))\n",
    "ipd.display(ipd.Audio(audio2.cpu().detach().numpy()[0], rate=44100))\n",
    "\n",
    "new_dur = torch.zeros(attn_hard_dur.shape)\n",
    "\n",
    "for i in range(attn_hard_dur.shape[1]):\n",
    "    if tokens[0,i] == 0:\n",
    "        new_dur[0,i] = int(attn_hard_dur[0,i] / 1.3)\n",
    "    else:\n",
    "        new_dur[0,i] = int(attn_hard_dur[0,i] / 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Fastpitch female 2 male attempt 1 - did not work\n",
    "###\n",
    "\n",
    "from nemo.collections.tts.models import FastPitchModel\n",
    "import torch\n",
    "from nemo.collections.tts.torch.helpers import beta_binomial_prior_distribution\n",
    "from nemo.utils import logging\n",
    "logging.setLevel(logging.ERROR)\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "\n",
    "fastpitch = FastPitchModel.restore_from(\"/home/jasoli/nemo/NeMo/FastPitch-Align-8051.nemo\").eval().cuda()\n",
    "from nemo.collections.tts.modules.hifigan_modules import Generator\n",
    "\n",
    "vocoder = Generator(\n",
    "    resblock=1,\n",
    "    upsample_rates=[8, 8, 4, 2],\n",
    "    upsample_kernel_sizes=[16, 16, 4, 4],\n",
    "    upsample_initial_channel=512,\n",
    "    resblock_kernel_sizes=[3, 7, 11],\n",
    "    resblock_dilation_sizes=[[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n",
    ")\n",
    "nemo_gen_keys = [k for k in vocoder.state_dict().keys()]\n",
    "adlr_gen_ckpt = torch.load(\"/home/jasoli/nemo/NeMo/hifigan_gen_nemo_8051_00524000.pt\")\n",
    "adlr_gen_keys = adlr_gen_ckpt.keys()\n",
    "\n",
    "new_nemo_ckpt = {nemo_key: adlr_gen_ckpt[adlr_key] for adlr_key, nemo_key in zip(adlr_gen_keys, nemo_gen_keys)}\n",
    "vocoder.load_state_dict(new_nemo_ckpt)\n",
    "vocoder = vocoder.cuda().eval()\n",
    "\n",
    "inp = (\"Who do I sound like?\")\n",
    "tokens = fastpitch.parse(inp)\n",
    "\n",
    "mels_pred, _, durs_predicted, log_durs_pred, pitch_pred, attn_soft, attn_logprob, attn_hard, attn_hard_dur, pitch = fastpitch(\n",
    "    text=tokens,\n",
    "    durs=None,\n",
    "    pitch=None,\n",
    "    speaker=None,\n",
    "    pace=1.0,\n",
    "    spec=None,\n",
    "    attn_prior=None,\n",
    "    mel_lens=None,\n",
    "    input_lens=torch.tensor(tokens.shape[1]).unsqueeze(0),\n",
    ")\n",
    "fig, ax = plt.subplots(figsize=[15,8])\n",
    "ax.imshow(mels_pred.cpu().detach().numpy()[0], origin=\"lower\")\n",
    "plt.show()\n",
    "\n",
    "audio = vocoder(x=mels_pred)\n",
    "\n",
    "ipd.display(ipd.Audio(audio.cpu().detach().numpy()[0], rate=44100))\n",
    "\n",
    "print(durs_predicted)\n",
    "mels_pred, _, durs_predicted, log_durs_pred, pitch_pred, attn_soft, attn_logprob, attn_hard, attn_hard_dur, pitch = fastpitch(\n",
    "    text=tokens,\n",
    "    durs=None,\n",
    "    pitch=None,\n",
    "    speaker=None,\n",
    "    pace=0.4,\n",
    "    spec=None,\n",
    "    attn_prior=None,\n",
    "    mel_lens=None,\n",
    "    input_lens=torch.tensor(tokens.shape[1]).unsqueeze(0).cuda(),\n",
    "    pitch_transform=-50,\n",
    ")\n",
    "fig, ax = plt.subplots(figsize=[15,8])\n",
    "ax.imshow(mels_pred.cpu().detach().numpy()[0], origin=\"lower\")\n",
    "plt.show()\n",
    "\n",
    "audio = vocoder(x=mels_pred)\n",
    "\n",
    "ipd.display(ipd.Audio(audio.cpu().detach().numpy()[0], rate=44100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# HiFigan cycle stationary consistency\n",
    "###\n",
    "\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import librosa.display\n",
    "\n",
    "y, sr = librosa.core.load(\"/home/jasoli/example.wav\", sr=None)\n",
    "stft_matrix = librosa.amplitude_to_db(np.abs(librosa.stft(y, n_fft=2048, window=\"hann\")))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[15,8])\n",
    "librosa.display.specshow(stft_matrix, sr=sr, hop_length=512, y_axis=\"log\", ax=ax)\n",
    "ipd.display(ipd.Audio(y, rate=44100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###\n",
    "# TorchScript export\n",
    "###\n",
    "from nemo.collections.tts.models import FastPitchModel, HifiGanModel\n",
    "\n",
    "fp = FastPitchModel.from_pretrained(\"tts_en_fastpitch\").cpu()\n",
    "# fp.export(\"fastpitch_ngc_ljs_pitch.ts\")\n",
    "# hf = HifiGanModel.from_pretrained(\"tts_hifigan\").cpu()\n",
    "# hf.export(\"tts_hifigan.ts\")\n",
    "\n",
    "# import torch\n",
    "\n",
    "# fp_ts = torch.jit.load('fastpitch_ngc_ljs_pitch.ts')\n",
    "\n",
    "# tokens = fp.parse(\"Hello.\")\n",
    "# # spec = fp_ts(tokens)\n",
    "# print(fp._parser._labels_map)\n",
    "# with open(\"fastpitch_ngc_ljs_mappings.txt\", \"w\") as f:\n",
    "#     for key, value in fp._parser._labels_map.items():\n",
    "#         f.write(f\"{value} {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###\n",
    "# ADLR checkpoint to .NeMo (only generator)\n",
    "###\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "\n",
    "from nemo.collections.tts.models import HifiGanModel\n",
    "from nemo.collections.tts.modules.hifigan_modules import Generator\n",
    "\n",
    "hf_conf = {}\n",
    "hf_conf[\"generator\"] = {\n",
    "    \"_target_\":\"nemo.collections.tts.modules.hifigan_modules.Generator\",\n",
    "    \"resblock\":1,\n",
    "    \"upsample_rates\":[8, 8, 4, 2],\n",
    "    \"upsample_kernel_sizes\":[16, 16, 4, 4],\n",
    "    \"upsample_initial_channel\":512,\n",
    "    \"resblock_kernel_sizes\":[3, 7, 11],\n",
    "    \"resblock_dilation_sizes\":[[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n",
    "}\n",
    "hf = HifiGanModel(hf_conf)\n",
    "\n",
    "# vocoder = Generator(\n",
    "#     resblock=1,\n",
    "#     upsample_rates=[8, 8, 4, 2],\n",
    "#     upsample_kernel_sizes=[16, 16, 4, 4],\n",
    "#     upsample_initial_channel=512,\n",
    "#     resblock_kernel_sizes=[3, 7, 11],\n",
    "#     resblock_dilation_sizes=[[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n",
    "# )\n",
    "nemo_gen_keys = [k for k in hf.state_dict().keys()]\n",
    "adlr_gen_ckpt = torch.load(\"/home/jasoli/nemo/NeMo/Hifigan_ADLR_Helen_44k_00570000.pt\")[\"generator\"]\n",
    "adlr_gen_keys = adlr_gen_ckpt.keys()\n",
    "\n",
    "new_nemo_ckpt = {nemo_key: adlr_gen_ckpt[adlr_key] for adlr_key, nemo_key in zip(adlr_gen_keys, nemo_gen_keys)}\n",
    "hf.load_state_dict(new_nemo_ckpt)\n",
    "hf.save_to(\"Hifigan_ADLR_Helen.nemo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# ADLR checkpoint to .NeMo (generator+disc)\n",
    "###\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from nemo.collections.tts.models import HifiGanModel\n",
    "\n",
    "\n",
    "adlr_gen_ckpt = torch.load(\n",
    "    \"/home/jasoli/nemo/NeMo/g_00587500\"\n",
    ")\n",
    "adlr_dis_ckpt = torch.load(\n",
    "    \"/home/jasoli/nemo/NeMo/do_00587500\"\n",
    ")\n",
    "save_path = \"/home/jasoli/nemo/NeMo/checkpoints/hifigan_from_adlr_in_nemo_helen_00587500.nemo\"\n",
    "model_config = OmegaConf.load('/home/jasoli/nemo/NeMo/examples/tts/my-hifigan-44k.yaml')\n",
    "\n",
    "del model_config[\"model\"][\"train_ds\"]\n",
    "del model_config[\"model\"][\"validation_ds\"]\n",
    "del model_config[\"exp_manager\"]\n",
    "\n",
    "model = HifiGanModel(cfg=model_config.model)\n",
    "nemo_gen_keys = [k for k in model.state_dict().keys() if \"generator\" in k]\n",
    "adlr_gen_keys = adlr_gen_ckpt[\"generator\"].keys()\n",
    "\n",
    "new_nemo_ckpt = {\n",
    "    nemo_key: adlr_gen_ckpt[\"generator\"][adlr_key] for adlr_key, nemo_key in zip(adlr_gen_keys, nemo_gen_keys)\n",
    "}\n",
    "\n",
    "for k in model.state_dict().keys():\n",
    "    if \"msd.discriminators\" in k:\n",
    "        new_nemo_ckpt[k] = adlr_dis_ckpt[\"msd\"][k.replace(\"msd.\", \"\")]\n",
    "    elif \"mpd.discriminators\" in k:\n",
    "        new_nemo_ckpt[k] = adlr_dis_ckpt[\"mpd\"][k.replace(\"mpd.\", \"\")]\n",
    "\n",
    "model.load_state_dict(OrderedDict(new_nemo_ckpt), strict=False)\n",
    "model.save_to(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "# Inference with FastPitch over dataset to prepare for hifigan finetuning\n",
    "###\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "from operator import ne\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from nemo.collections.asr.data.audio_to_text import AudioToCharWithPriorAndPitchDataset\n",
    "from nemo.collections.tts.models import FastPitchModel\n",
    "\n",
    "def generate_samples():\n",
    "    prior_folder = \"/data/speech/Helen/FasPitch_align/\"\n",
    "    nemo_checkpoint = \"/home/jasoli/nemo/NeMo/checkpoints/FastPitch-Align-Helen.nemo\"\n",
    "    manifest_filepath = \"/data/speech/Helen/helen_train.json\"\n",
    "#     manifest_filepath = \"/data/speech/Helen/helen_val.json\"\n",
    "    syn_mel_folder = Path(\"/data/speech/Helen/FastPitch_align_syn_mel\")\n",
    "    \n",
    "    model = FastPitchModel.restore_from(nemo_checkpoint, map_location=\"cpu\").cuda(1).eval()\n",
    "    vocab_dict = {\n",
    "        \"notation\": \"phonemes\",\n",
    "        \"punct\": True,\n",
    "        \"spaces\": True,\n",
    "        \"stresses\": True,\n",
    "        \"add_blank_at\": None,\n",
    "        \"pad_with_space\": True,\n",
    "        \"chars\": True,\n",
    "        \"improved_version_g2p\": True,\n",
    "    }\n",
    "    dataset = AudioToCharWithPriorAndPitchDataset(\n",
    "        sup_data_path=prior_folder,\n",
    "        n_window_stride=512,\n",
    "        n_window_size=2048,\n",
    "        manifest_filepath=manifest_filepath,\n",
    "        sample_rate=44100,\n",
    "        pitch_fmin=model._cfg.pitch_fmin,\n",
    "        pitch_fmax=model._cfg.pitch_fmax,\n",
    "        pitch_avg=model._cfg.pitch_avg,\n",
    "        pitch_std=model._cfg.pitch_std,\n",
    "        vocab=vocab_dict,\n",
    "    )\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset, batch_size=32, collate_fn=dataset.collate_fn, num_workers=8, shuffle=False\n",
    "    )\n",
    "\n",
    "    if not syn_mel_folder.exists():\n",
    "        syn_mel_folder.mkdir()\n",
    "\n",
    "    i = 0\n",
    "    for batch in dataloader:\n",
    "        audio, audio_lens, text, text_lens, attn_prior, _, _ = batch\n",
    "        with torch.no_grad():\n",
    "            mels, spec_len = model.preprocessor(input_signal=audio.cuda(1), length=audio_lens.cuda(1))\n",
    "            mels_pred, _, _, _, _, _, _, _, attn_hard_dur, _ = model(\n",
    "                text=text.cuda(1),\n",
    "                durs=None,\n",
    "                pitch=None,\n",
    "                speaker=None,\n",
    "                pace=1.0,\n",
    "                spec=mels,\n",
    "                attn_prior=attn_prior.cuda(1),\n",
    "                mel_lens=spec_len,\n",
    "                input_lens=text_lens.cuda(1),\n",
    "            )\n",
    "        mels = mels_pred.cpu().numpy()\n",
    "        for j, mel in enumerate(mels):\n",
    "            mel_length = int(torch.sum(attn_hard_dur[j]))\n",
    "            second_half = \"train_{}\" if \"train\" in manifest_filepath else \"val_{}\"\n",
    "            save_path = syn_mel_folder / second_half.format(i)\n",
    "            np.save(save_path, mel[:, :mel_length])\n",
    "            if i < 10:\n",
    "                print(save_path)\n",
    "            i += 1\n",
    "\n",
    "\n",
    "def make_new_json():\n",
    "    manifest_filepath = \"/data/speech/Helen/helen_train.json\"\n",
    "    new_json_filepath = \"/data/speech/Helen/helen_train_synmel.json\"\n",
    "#     manifest_filepath = \"/data/speech/Helen/helen_val.json\"\n",
    "#     new_json_filepath = \"/data/speech/Helen/helen_val_synmel.json\"\n",
    "    syn_mel_folder = Path(\"/data/speech/Helen/FastPitch_align_syn_mel\")\n",
    "\n",
    "    with open(manifest_filepath, \"r\") as data_in:\n",
    "        lines = data_in.readlines()\n",
    "\n",
    "    with open(new_json_filepath, \"w\") as data_out:\n",
    "        for i, line in enumerate(lines):\n",
    "            in_line = json.loads(line)\n",
    "            second_half = \"train_{}.npy\" if \"train\" in manifest_filepath else \"val_{}.npy\"\n",
    "            save_path = syn_mel_folder / second_half.format(i)\n",
    "            in_line[\"mel_filepath\"] = str(save_path)\n",
    "            data_out.write(json.dumps(in_line) + '\\n')\n",
    "\n",
    "generate_samples()\n",
    "make_new_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hifigan finetuning\n",
    "\n",
    "import json\n",
    "\n",
    "# with open(\"/data/speech/Helen/helen_train.json\", \"r\") as data_in:\n",
    "with open(\"/data/speech/NvidiaCustomTTSFinetuning/trimmed_sirisha35/manifest35_15mins.json\", \"r\") as data_in:\n",
    "    lines = data_in.readlines()\n",
    "\n",
    "# with open(\"/data/speech/Helen/helen_train_synmel.json\", \"w\") as data_out:\n",
    "with open(\"/data/speech/NvidiaCustomTTSFinetuning/trimmed_sirisha35/manifest35_syn_15mins.json\", \"w\") as data_out:\n",
    "    for i, line in enumerate(lines):\n",
    "        in_line = json.loads(line)\n",
    "        # in_line[\"mel_filepath\"] = f\"/data/speech/Helen/FastPitch_align_syn_mel/train_{i}.npy\"\n",
    "        in_line[\"mel_filepath\"] = f\"/data/speech/NvidiaCustomTTSFinetuning/trimmed_sirisha35/audio_syn_15min/train_{i}.npy\"\n",
    "        data_out.write(json.dumps(in_line)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###\n",
    "# Inference (FP+HiFiGAN)\n",
    "###\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "\n",
    "from nemo.collections.asr.data.audio_to_text import _AudioTextDataset\n",
    "from nemo.collections.tts.models import FastPitchModel, HifiGanModel\n",
    "from nemo.collections.tts.modules.hifigan_modules import Generator\n",
    "\n",
    "\n",
    "fp = FastPitchModel.restore_from(\"/home/jasoli/nemo/NeMo/checkpoints/Helen/FastPitch-Align-Helen.nemo\", map_location=\"cpu\")\n",
    "# fp = FastPitchModel.restore_from(\"/home/jasoli/nemo/NeMo/FinetuningDemo/Sirisha_15_mins/FastPitch/2021-09-28_22-20-50/checkpoints/FastPitch.nemo\")\n",
    "# fp = FastPitchModel.load_from_checkpoint(\"/home/jasoli/nemo/NeMo/checkpoints/JonCkpts/FastPitchNoMixJon.ckpt\")\n",
    "fp = fp.cuda(1).eval()\n",
    "\n",
    "# vocoder = Generator(\n",
    "#     resblock=1,\n",
    "#     upsample_rates=[8, 8, 4, 2],\n",
    "#     upsample_kernel_sizes=[16, 16, 4, 4],\n",
    "#     upsample_initial_channel=512,\n",
    "#     resblock_kernel_sizes=[3, 7, 11],\n",
    "#     resblock_dilation_sizes=[[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n",
    "# )\n",
    "# nemo_gen_keys = [k for k in vocoder.state_dict().keys()]\n",
    "# # adlr_gen_ckpt = torch.load(\"/home/jasoli/nemo/NeMo/Hifigan_ADLR_male_44k_00632500\")\n",
    "# # adlr_gen_ckpt = torch.load(\"/home/jasoli/nemo/NeMo/Hifigan_ADLR_male_44k_00632500\")[\"generator\"]\n",
    "# adlr_gen_ckpt = torch.load(\"/home/jasoli/nemo/NeMo/Hifigan_ADLR_Helen_44k_00570000.pt\")[\"generator\"]\n",
    "# adlr_gen_keys = adlr_gen_ckpt.keys()\n",
    "\n",
    "# new_nemo_ckpt = {nemo_key: adlr_gen_ckpt[adlr_key] for adlr_key, nemo_key in zip(adlr_gen_keys, nemo_gen_keys)}\n",
    "# vocoder.load_state_dict(new_nemo_ckpt)\n",
    "\n",
    "# vocoder = HifiGanModel.load_from_checkpoint(\"/home/jasoli/nemo/NeMo/checkpoints/JonCkpts/HifiGanMix.ckpt\")\n",
    "vocoder = HifiGanModel.restore_from(\"/home/jasoli/nemo/NeMo/checkpoints/hifiganfinetune/HifiGan/2021-09-17_21-15-08/checkpoints/HifiGan.nemo\", map_location=\"cpu\")\n",
    "# whatever = torch.load(\"/home/jasoli/nemo/NeMo/HifiGan--val_loss=0.26-epoch=246-last.ckpt\")\n",
    "# vocoder = HifiGanModel(cfg=whatever[\"hyper_parameters\"])\n",
    "# vocoder.load_state_dict(whatever[\"state_dict\"])\n",
    "vocoder = vocoder.cuda(1).eval().half()\n",
    "\n",
    "vocoder2 = HifiGanModel.restore_from(\"/home/jasoli/nemo/NeMo/checkpoints/hifiganfinetune_try2/HifiGan/2021-09-29_20-54-05/checkpoints/HifiGan.nemo\", map_location=\"cpu\")\n",
    "vocoder2 = vocoder.cuda(1).eval().half()\n",
    "\n",
    "\n",
    "import json\n",
    "import soundfile as sf\n",
    "import IPython.display as ipd\n",
    "\n",
    "with open(\"/data/speech/LJSpeech/nvidia_ljspeech_test.json\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "lines = [json.loads(l) for l in lines]\n",
    "\n",
    "for i, l in enumerate(lines):\n",
    "    tokens = fp.parse(l['text']).cuda(1)\n",
    "    with torch.no_grad():\n",
    "        spectrogram = fp.generate_spectrogram(tokens=tokens)\n",
    "    #     audio = vocoder(x=spectrogram.half()).squeeze(1)  # x for generator\n",
    "        audio = vocoder(spec=spectrogram.half()).squeeze(1)  # spec for hifiganmodel\n",
    "        audio2 = vocoder2(spec=spectrogram.half()).squeeze(1)  # spec for hifiganmodel\n",
    "    print(i)\n",
    "    print((audio==audio2).all())\n",
    "    ipd.display(ipd.Audio(audio.float().cpu().numpy(), rate=44100))\n",
    "    ipd.display(ipd.Audio(audio2.float().cpu().numpy(), rate=44100))\n",
    "#     sf.write(f'sample_{i}.wav', audio.float().cpu().numpy().squeeze(0), 44100)\n",
    "    if i > 10:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###\n",
    "# Mix Sally duration with Helen model\n",
    "###\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "from nemo.collections.tts.models import FastPitchModel\n",
    "from nemo.core.config import hydra_runner\n",
    "import IPython.display as ipd\n",
    "from nemo.core.classes import typecheck\n",
    "\n",
    "typecheck.set_typecheck_enabled(enabled=False)\n",
    "\n",
    "\n",
    "config = FastPitchModel.restore_from(\"/home/jasoli/nemo/NeMo/checkpoints/Helen/FastPitch-Align-Helen.nemo\", return_config=True)\n",
    "config.vocab = config.train_ds.dataset.vocab\n",
    "config.vocab.improved_version_g2p = True\n",
    "config.vocab.phoneme_dict_path = \"/data/cmudict-updatedgtctokkio\"\n",
    "del config.train_ds\n",
    "del config.validation_ds\n",
    "fastpitch = FastPitchModel(cfg=config)\n",
    "sally_checkpoint = torch.load(\"/home/jasoli/nemo/NeMo/checkpoints/Sally/model_weights.ckpt\")\n",
    "helen_checkpoint = torch.load(\"/home/jasoli/nemo/NeMo/checkpoints/Helen/model_weights.ckpt\")\n",
    "\n",
    "# print(config.train_ds.dataset)\n",
    "model_params = {}\n",
    "for n, p in fastpitch.named_parameters():\n",
    "    model_params[n] = ''\n",
    "for key, value in sally_checkpoint.items():\n",
    "    if 'encoder' in key:\n",
    "        new_key_pos = key.find(\"encoder\")\n",
    "        new_key = key[:new_key_pos+len(\"encoder\")]+\"_sally\"+key[new_key_pos+len(\"encoder\"):]\n",
    "        model_params[new_key] = value\n",
    "    elif 'fastpitch.duration_predictor' in key:\n",
    "        new_key_pos = key.find(\"duration_predictor\")\n",
    "        new_key = key[:new_key_pos+len(\"duration_predictor\")]+\"_sally\"+key[new_key_pos+len(\"duration_predictor\"):]\n",
    "        model_params[new_key] = value\n",
    "for key, value in helen_checkpoint.items():\n",
    "    if 'encoder' in key:\n",
    "        new_key_pos = key.find(\"encoder\")\n",
    "        new_key = key[:new_key_pos+len(\"encoder\")]+\"_helen\"+key[new_key_pos+len(\"encoder\"):]\n",
    "        model_params[new_key] = value\n",
    "    elif 'fastpitch.duration_predictor' in key:\n",
    "        new_key_pos = key.find(\"duration_predictor\")\n",
    "        new_key = key[:new_key_pos+len(\"duration_predictor\")]+\"_helen\"+key[new_key_pos+len(\"duration_predictor\"):]\n",
    "        model_params[new_key] = value\n",
    "    else:\n",
    "        model_params[key] = value\n",
    "\n",
    "fastpitch.load_state_dict(model_params)\n",
    "fastpitch.save_to(\"/home/jasoli/nemo/NeMo/checkpoints/Fastpitch-Align-SallySpeed-HelenVoice.nemo\")\n",
    "\n",
    "## INFERENCE\n",
    "from nemo.collections.tts.models import HifiGanModel\n",
    "from nemo.collections.tts.modules.hifigan_modules import Generator\n",
    "\n",
    "# vocoder = HifiGanModel.restore_from(\"/home/jasoli/nemo/NeMo/checkpoints/hifigan_from_adlr_in_nemo_helen_00587500.nemo\")\n",
    "# vocoder = HifiGanModel.restore_from(\"/home/jasoli/nemo/NeMo/checkpoints/hifiganfinetune/HifiGan/2021-09-17_21-15-08/checkpoints/HifiGan.nemo\")\n",
    "# whatever = torch.load(\"/home/jasoli/nemo/NeMo/HifiGan--val_loss=0.26-epoch=246-last.ckpt\")\n",
    "# vocoder = HifiGanModel(cfg=whatever[\"hyper_parameters\"])\n",
    "# vocoder.load_state_dict(whatever[\"state_dict\"])\n",
    "\n",
    "vocoder = Generator(\n",
    "    resblock=1,\n",
    "    upsample_rates=[8, 8, 4, 2],\n",
    "    upsample_kernel_sizes=[16, 16, 4, 4],\n",
    "    upsample_initial_channel=512,\n",
    "    resblock_kernel_sizes=[3, 7, 11],\n",
    "    resblock_dilation_sizes=[[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n",
    ")\n",
    "nemo_gen_keys = [k for k in vocoder.state_dict().keys()]\n",
    "adlr_gen_ckpt = torch.load(\"/home/jasoli/nemo/NeMo/checkpoints/helen_44khz_g_00609000ftv2.pt\")[\"generator\"]\n",
    "adlr_gen_keys = adlr_gen_ckpt.keys()\n",
    "\n",
    "new_nemo_ckpt = {nemo_key: adlr_gen_ckpt[adlr_key] for adlr_key, nemo_key in zip(adlr_gen_keys, nemo_gen_keys)}\n",
    "vocoder.load_state_dict(new_nemo_ckpt)\n",
    "\n",
    "vocoder = vocoder.cuda().eval().half()\n",
    "fastpitch = fastpitch.cuda().eval()\n",
    "\n",
    "tokkio_samples = [\n",
    "    \"Hi! Welcome to Shannon's cafe! How can I help you?\",\n",
    "    \"Ok! Would you be interested in some toppings like bacon or fried onions?\",\n",
    "#     \"Ok! Anything else?\",\n",
    "#     \"We have a double-protein burger. Would you like one?\",\n",
    "#     \"Its protein comes from two beef patties.\",\n",
    "#     \"We have blackbean patties for vegetarian options. Which one would you like?\",\n",
    "#     \"Got it! Anything else?\",\n",
    "#     \"These are the 4 sides and we have 3 vegetarian options. What would you like?\",\n",
    "#     \"Sure! Care to make it interesting with garlic on it?\",\n",
    "#     \"Got it! Medium french fries with garlic.\",\n",
    "#     \"Got it!\",\n",
    "#     \"Sure!\",\n",
    "    \"We have cobb salad, market salad, fruit salad, and side salad. Side salad goes well with the cheeseburger. What would you prefer?\",\n",
    "#     \"It is sunny out there! Would you like some lemonade?\",\n",
    "#     \"Great! Anything else?\",\n",
    "#     \"We have diet and regular. Which one would you like?\",\n",
    "#     \"Adding one regular coke. Anything else?\",\n",
    "    \"I've added a cheeseburger with bacon and fried onions, a vegetarian protein burger, medium onion rings, medium french fries with extra garlic, side salad, lemonade and a regular coke to your cart.\",\n",
    "    \"Your food will be ready shortly. Thank you\",\n",
    "]\n",
    "\n",
    "\n",
    "def id_to_chars(arr):\n",
    "    return [\"/\".join([fastpitch.vocab._id2label[i] for i in arr])]\n",
    "\n",
    "all_f0 = np.array([])\n",
    "for text in tokkio_samples:\n",
    "    tokens = fastpitch.parse(text).cuda()\n",
    "    with torch.no_grad():\n",
    "#         for i in [0., 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
    "        for pace in np.arange(1.15, 1.3, 0.05):\n",
    "            spectrogram = fastpitch.generate_spectrogram(tokens=tokens, helen_mix=0.2, pace=pace)\n",
    "            audio = vocoder(x=spectrogram.half()).squeeze(1)  # x for generator\n",
    "    #         audio = vocoder(spec=spectrogram.half()).squeeze(1)  # spec for hifiganmodel\n",
    "            print(text)\n",
    "            print(id_to_chars(tokens[0]))\n",
    "            print(pace)\n",
    "            ipd.display(ipd.Audio(audio.float().cpu().numpy(), rate=44100))\n",
    "        \n",
    "            f0, voiced_flag, voiced_probs = librosa.pyin(audio.float().cpu().numpy().squeeze(), fmin=80, fmax=640, frame_length=2048, sr=44100, fill_na=0.)\n",
    "            all_f0 = np.concatenate((all_f0,f0[f0!=0].flatten()))\n",
    "\n",
    "print(f\"pitch mean: {np.mean(all_f0)}\")\n",
    "print(f\"pitch std: {np.std(all_f0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TTS Finetuning\n",
    "\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "import random\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import librosa.display\n",
    "\n",
    "filelist = Path(\"/data/speech/NvidiaCustomTTSFinetuning/Sirisha\").glob(\"*.wav\")\n",
    "filelist = [t for t in filelist]\n",
    "random.shuffle(filelist)\n",
    "for f in filelist:\n",
    "    y, sr = librosa.load(f, sr=None)\n",
    "    trimmed, _ = librosa.effects.trim(y, top_db=60, frame_length=1024, hop_length=256)\n",
    "    print(len(y)-len(trimmed))\n",
    "    assert sr == 44100\n",
    "    ipd.display(ipd.Audio(trimmed, rate=sr))\n",
    "    mag = np.abs(librosa.stft(trimmed, n_fft=2048, window=\"hann\"))\n",
    "    stft_matrix = librosa.amplitude_to_db(mag)\n",
    "    fig, ax = plt.subplots(figsize=[8,6])\n",
    "    librosa.display.specshow(stft_matrix, sr=sr, hop_length=512, y_axis=\"log\", ax=ax)\n",
    "    plt.show()\n",
    "    energy = np.sum(stft_matrix, axis=1)\n",
    "    np.sum(mag, axis=0)\n",
    "    plt.plot(np.sum(mag, axis=0))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### VCTK reading\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "harvard_sentences = set()\n",
    "with open(\"/home/jasoli/harvard.txt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        harvard_sentences.add(line)\n",
    "        \n",
    "VCTK_sentences = set()\n",
    "for file in Path(\"/data/speech/VCTK/txt\").glob(\"*/*.txt\"):\n",
    "    with open(file) as f:\n",
    "        text = f.read()\n",
    "\n",
    "#     file_number = file.stem.split(\"_\")[1]\n",
    "#     if file_number in VCTK_sentences:\n",
    "#         assert text == VCTK_sentences[file_number][\"data\"], f\"{text}!={VCTK_sentences[file_number]['data']}|{file}|{VCTK_sentences[file_number]['file']}\"\n",
    "#     else:\n",
    "#         VCTK_sentences[file_number] = {'data': text, 'file': file}\n",
    "    VCTK_sentences.add(text.strip())\n",
    "    assert text not in harvard_sentences, f\"{file}|{text}| was in harvard\"\n",
    "\n",
    "sentences = sorted(VCTK_sentences)\n",
    "# sentence_length = [len(s) for s in sentences]\n",
    "# hist = {}\n",
    "# for l in sentence_length:\n",
    "#     if l not in hist:\n",
    "#         hist[l] = 1\n",
    "#     else:\n",
    "#         hist[l] += 1\n",
    "filtered_sentences = []\n",
    "for s in sentences:\n",
    "    if len(s) > 130 or len(s) < 20:\n",
    "        continue\n",
    "    if any([c.isupper() for c in s[1:]]):\n",
    "        continue\n",
    "    filtered_sentences.append(s)\n",
    "\n",
    "final_sentences = []\n",
    "for i, s in enumerate(filtered_sentences):\n",
    "    if i % 20 == 0:\n",
    "        final_sentences.append(s)\n",
    "        \n",
    "for s in final_sentences:\n",
    "    print(s)\n",
    "print(len(final_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hifigan checkpoint -> nemo\n",
    "\n",
    "from nemo.collections.tts.models import HifiGanModel\n",
    "\n",
    "model = HifiGanModel.load_from_checkpoint(\"/home/jasoli/nemo/NeMo/examples/tts/nemottsmodels/1.3.1/HifiGan-UniversalHiFi.ckpt\")\n",
    "model.save_to(\"/home/jasoli/nemo/NeMo/examples/tts/nemottsmodels/1.3.1/HifiGan-UniversalHiFi.nemo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GTC Finetuning Mixing filelists\n",
    "\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "baseline_speaker_files = []\n",
    "new_speaker_files = []\n",
    "\n",
    "stems = set()\n",
    "\n",
    "with open(\"/data/speech/HiFiTTS/8051_manifest_clean_train.json\", \"r\") as f:\n",
    "    for f in f.readlines():\n",
    "        baseline_speaker_files.append(json.loads(f))\n",
    "        baseline_speaker_files[-1][\"text\"] = baseline_speaker_files[-1][\"text_normalized\"]\n",
    "        del baseline_speaker_files[-1][\"text_no_preprocessing\"]\n",
    "        del baseline_speaker_files[-1][\"text_normalized\"]\n",
    "        baseline_speaker_files[-1][\"speaker\"] = 0\n",
    "        \n",
    "#         stem = Path(baseline_speaker_files[-1][\"audio_filepath\"]).stem\n",
    "#         assert stem not in stems\n",
    "#         stems.add(stem)\n",
    "\n",
    "\n",
    "with open(\"/data/speech/NvidiaCustomTTSFinetuning/trimmed_sirisha35/manifest35.json\", \"r\") as f:\n",
    "    for f in f.readlines():\n",
    "        new_speaker_files.append(json.loads(f))\n",
    "        new_speaker_files[-1][\"speaker\"] = 1\n",
    "        new_speaker_files[-1][\"text\"]+=(\".\")\n",
    "        \n",
    "#         stem = Path(new_speaker_files[-1][\"audio_filepath\"]).stem\n",
    "#         assert stem not in stems\n",
    "#         stems.add(stem)\n",
    "\n",
    "np.random.shuffle(new_speaker_files)\n",
    "np.random.shuffle(baseline_speaker_files)\n",
    "baseline_speaker_files = baseline_speaker_files[:5000]  # Take first 5k\n",
    "\n",
    "with open(\"/data/speech/NvidiaCustomTTSFinetuning/trimmed_sirisha35/manifest_8051mix_44mins.json\", \"w\") as f:\n",
    "    for ridx, original_record in enumerate(baseline_speaker_files):\n",
    "        new_speaker_record = new_speaker_files[ridx % len(new_speaker_files)]\n",
    "        f.write(json.dumps(original_record) + \"\\n\")\n",
    "        f.write(json.dumps(new_speaker_record) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GTC Finetuning check dataset\n",
    "\n",
    "from nemo.collections.asr.data.audio_to_text import AudioToCharWithPriorAndPitchDataset\n",
    "params = {\n",
    "    \"manifest_filepath\": \"/data/speech/NvidiaCustomTTSFinetuning/trimmed_oleksii35/manifest_8051mix_40mins.json\",\n",
    "    \"int_values\": False,\n",
    "    \"normalize\": True,\n",
    "    \"sample_rate\": 44100,\n",
    "    \"trim\": False,\n",
    "    \"sup_data_path\": \"/data/speech/NvidiaCustomTTSFinetuning/trimmed_oleksii35/pitch_prior\",\n",
    "    \"n_window_stride\": 512,\n",
    "    \"n_window_size\": 2048,\n",
    "    \"pitch_fmin\": 80,\n",
    "    \"pitch_fmax\": 256,\n",
    "    \"pitch_avg\": 107.3371734925776,\n",
    "    \"pitch_std\": 19.38637136174705,\n",
    "    \"vocab\":{\n",
    "        \"notation\": \"phonemes\",\n",
    "        \"punct\": True,\n",
    "        \"spaces\": True,\n",
    "        \"stresses\": True,\n",
    "        \"add_blank_at\": None,\n",
    "        \"pad_with_space\": True,\n",
    "        \"chars\": True,\n",
    "        \"improved_version_g2p\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "dataset = AudioToCharWithPriorAndPitchDataset(**params)\n",
    "\n",
    "filelist = []\n",
    "with open(\"/data/speech/NvidiaCustomTTSFinetuning/trimmed_oleksii35/manifest_8051mix_40mins.json\", \"r\") as f:\n",
    "    for f in f.readlines():\n",
    "        filelist.append(json.loads(f))\n",
    "\n",
    "audio, audio_len, text, text_len, attn_prior, pitch, speaker = dataset[0]\n",
    "\n",
    "print(text)\n",
    "print(text_len)\n",
    "print(filelist[0][\"text\"])\n",
    "print(len(filelist[0][\"text\"]))\n",
    "\n",
    "print(dataset.vocab._label2id)\n",
    "print(dataset.vocab._id2label)\n",
    "[\"/\".join([dataset.vocab._id2label[i] for i in text])]\n",
    "\n",
    "def id_to_chars(arr):\n",
    "    return [\"/\".join([dataset.vocab._id2label[i] for i in arr])]\n",
    "\n",
    "id_to_chars(dataset.vocab.encode(\"nemo\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
